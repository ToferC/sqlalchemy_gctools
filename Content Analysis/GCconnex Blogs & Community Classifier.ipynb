{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using machine learning to classify the community of GCconnex blogs \n",
    "### (and other content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter more details here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports for basic Python\n",
    "\n",
    "import sqlalchemy as sq\n",
    "import pymysql\n",
    "\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "sq.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Gensim\n",
    "\n",
    "from gensim import corpora, models\n",
    "from gensim.utils import simple_preprocess, lemmatize\n",
    "from gensim.parsing.preprocessing import STOPWORDS as STOPWORDS\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# create English stop word list\n",
    "en_stops = set(stopwords.words('english'))\n",
    "fr_stops = set(stopwords.words('french'))\n",
    "\n",
    "# Add certain additional stop words\n",
    "public_service_stops = '''public service canada work http \n",
    "https travail gcconnex url'''.split()\n",
    "\n",
    "# Set up stemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text Preprocessing functions\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def tokenize(text):\n",
    "    return [lemmatize_stemming(token) for token in tokenizer.tokenize(str(text))\n",
    "            if token not in STOPWORDS if token not in fr_stops\n",
    "           if token not in public_service_stops if len(token) > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMport SKlearn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set data paths\n",
    "\n",
    "data_path = '/home/chris/data/'\n",
    "output_path = '/home/chris/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utility scripts\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open(os.path.join(data_path, \"{}.pkl\".format(name)), 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(os.path.join(data_path, \"{}.pkl\".format(name)), 'rb') as f:\n",
    "        return(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading data\n",
    "# See SQLAlchemy GCconnex Classifier - April 27 for initial loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load blogs\n",
    "\n",
    "raw_blogs = pd.DataFrame.from_csv(os.path.join(\n",
    "    data_path, \"gcconnex_blogs_info.csv\"))\n",
    "\n",
    "blog_tokens = load_obj(\"blog_tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load communities data\n",
    "\n",
    "# dictionary of communities and associated tags\n",
    "communities = load_obj(\"communities\")\n",
    "\n",
    "# dictionary of community count for each blog\n",
    "blog_communities = load_obj(\"blog_communities\")\n",
    "\n",
    "# list of names for each community\n",
    "community_names = load_obj(\"community_names\")\n",
    "\n",
    "# array of binary (0,1) labels for each community for each blog\n",
    "community_labels = load_obj(\"binarized_labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load dictionaries\n",
    "\n",
    "content_dictionary = corpora.Dictionary.load(\n",
    "    os.path.join(data_path, \"gcconnex_blogs_content_dictionary\"))\n",
    "\n",
    "tag_dictionary = corpora.Dictionary.load(\n",
    "    os.path.join(data_path, \"gcconnex_blogs_tags_dictionary\"))\n",
    "\n",
    "community_tags_dictionary = corpora.Dictionary.load(\n",
    "    os.path.join(data_path, \"community_tags_dictionary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load LDA model(s)\n",
    "\n",
    "lda_model = load_obj(\"gcconnex_blogs_lda_model.pkl\")\n",
    "\n",
    "lda_model_tfidf = load_obj(\"gcconnex_blogs_tfidf_lda_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load vectorizers based on the tokenized and lemmatized blogs\n",
    "\n",
    "count_vectorizer = load_obj(\"count_vectorizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load classifiers\n",
    "\n",
    "multilabel_clf = load_obj(\"multilabel_clf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blueprint = '''\n",
    "Blueprint 2020 is a vision for a world-class Public Service equipped to serve Canada and Canadians now and into the future.\n",
    "\n",
    "With around a quarter-million employees, the Public Service of Canada is the largest employer in the country. We work across more than a hundred departments and agencies, delivering important services to Canadians – from providing old age security and employment insurance benefits to protecting Canadian sovereignty to helping families save for higher education. We are responsible for regulating the safety of food and drugs, undertaking research and development to protect our shared environment, promoting Canada’s national interests around the world, and developing economic, trade and energy policies, among many other duties. We make a difference in the lives of Canadians every day. To ensure continued excellence in public service requires us to always ask how we can improve both our performance and our value to Canadians. The Blueprint 2020 initiative was devised to help us ask these questions, and to allow us to build tomorrow’s Public Service together.\n",
    "\n",
    "Since June 2013, tens of thousands of public servants have shared their views on what it takes to ensure public service excellence. This input is redefining how we work and is making engagement part of our shared culture.\n",
    "\n",
    "A team of employees working at the Canada Border Service Agency\n",
    "By making a space to discuss our passion for public service and to act on our ideas for improvement, we are now well on our way to turning vision into reality. Building the Public Service of the future is a process that requires us all to commit to action, and to dedicate the time and effort necessary to see it through. We are all personally accountable for bringing about real change and realizing our ambitious goals.\n",
    "\n",
    "For some examples of innovations that have arisen as a result of engaged employees working together to deliver results for Canadians, please check out the Clerk of the Privy Council’s latest Annual Report to the Prime Minister on the Public Service of Canada.\n",
    "\n",
    "Blueprint 2020 Principles:\n",
    "The Blueprint 2020 vision is guided by four principles, as outlined in the document Blueprint 2020: Getting Started – Getting Your Views, that help examine how work is done in the federal Public Service:\n",
    "\n",
    "An open and networked environment that engages citizens and partners for the public good.\n",
    "A whole-of-government approach that enhances service delivery and value for money.\n",
    "A modern workplace that makes smart use of new technologies to improve networking, access to data and customer service.\n",
    "A capable, confident and high-performing workforce that embraces new ways of working and mobilizes the diversity of talent to serve the country’s evolving needs.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "asprof = '''About our Administrative Professionals Competency Profiles\n",
    "\n",
    "The AS Community Initiative has decided to align the four core competencies into a comprehensive document that enables integrated human resources practices. The objective of this approach is to:\n",
    "\n",
    "• motivate the employees to take charge of their future and better understand what is expected of them\n",
    "\n",
    "• help managers evaluate the core competencies effectively and fairly through the performance management cycle\n",
    "\n",
    "• accelerate, improve and shape human resources processes\n",
    "\n",
    "Content\n",
    "\n",
    "The definition of a competency is the knowledge, skills, abilities and behaviours that employees use to perform their work.\n",
    "\n",
    "Each behavioural competency presented in this document has a definition and a progressive scale of effective behaviours. The definition explains what the competency means in general, and the progressive scale has five\n",
    "\n",
    "different levels that identify the expected behaviours to be demonstrated by the Treasury Board of Canada Secretariat’s AS and CR employees. The more you progress in the scale, the more your behaviours require a\n",
    "\n",
    "broader perspective to take action on more complex situations.\n",
    "\n",
    "Using the Administrative Professionals Competency Profiles\n",
    "\n",
    "In order to fully understand the competencies in this document, it is important to first read the definition and then look at the progression of scale. Doing this will give you a complete picture of what is expected for each\n",
    "\n",
    "competency.\n",
    "\n",
    "There are two components to a competency: definition and scale. The definition explains what the competency means. This explanation provides a common language that everyone in the department can use. Each\n",
    "\n",
    "competency also has a progressive scale, which is divided into five levels with a description of what behaviours are expected throughout. Each competency scale is cumulative, which means that, although behaviours at\n",
    "\n",
    "lower levels are not repeated at higher levels, they nonetheless apply. As you progress through the scale, the expected behaviours grow from reactive to strategic. A reactive behaviour means that the employee is\n",
    "\n",
    "responsive to a situation and may be prompted by someone else, such as a supervisor or a client. A strategic behaviour takes into consideration a broader scope in order to plan and take proactive action in a complex\n",
    "\n",
    "situation.\n",
    "\n",
    "NOTE: These are general guidelines. AS and CR employees perform a wide range of duties. Therefore, the levels indicated will vary depending on the position. For example, some AS-04 positions\n",
    "\n",
    "may, in fact, require a combination of behaviours that have been linked to the AS-03 and AS-5 levels in this document. However, you can still use the competency profile to identify what would be the\n",
    "\n",
    "next logical behavior for you to work on.\n",
    "\n",
    "Questions\n",
    "\n",
    "If you have any questions related to the understanding of these Administrative Professionals Competencies Profiles, we invite you to send an email to the AS-Initiative- AS mailbox.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keyword extraction using slightly modified nltk_rake\n",
    "\n",
    "import nltk_rake\n",
    "\n",
    "rake = nltk_rake.RakeKeywordExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keywords = rake.extract(asprof, incl_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keywords[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# manual pre-processing of text - Need to build this into pipeline\n",
    "\n",
    "bp_tokens = tokenize(asprof)\n",
    "bp_tokens = \" \".join(bp_tokens)\n",
    "\n",
    "bp_tokens_count = count_vectorizer.transform([bp_tokens])\n",
    "bp_tokens_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_prob = multilabel_clf.predict_proba(bp_tokens_count)\n",
    "predict = multilabel_clf.predict(bp_tokens_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a strong read along several of the communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Script to generate simple community outputs\n",
    "\n",
    "def predict_communities(predict_array):\n",
    "    for i, element in enumerate(np.nditer(predict_array)):\n",
    "        print(\"{}: {}\".format(community_names[i], element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Communities with probabilities\n",
    "\n",
    "predict_communities(predict_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Communities with binary tags\n",
    "\n",
    "predict_communities(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "Now to use the pipeline we developed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up pipeline to tokenize, vectorize and classify data\n",
    "\n",
    "classification_pipeline = Pipeline([\n",
    "    ('vectorizer', count_vectorizer),\n",
    "    ('clf', multilabel_clf),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict = classification_pipeline.predict([blueprint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_communities(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of note, because we aren't doing the advanced tokenization, the pipeline is showing much weaker signals for each of the communities.  We need to either embed our specific tokenization (or TFidf) into the pipeline or set up a custom job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning\n",
    "\n",
    "Set up for using Keras and developing a deep learning algorithm for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import Keras libraries\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Need to build a better model here - CNN\n",
    "# alternating dense and non-linear layers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(15, input_dim=51658, activation=\"relu\", kernel_initializer=\"normal\"))\n",
    "model.add(Dense(15, activation=\"sigmoid\", kernel_initializer=\"normal\"))\n",
    "\n",
    "# Compile model\n",
    "print(\"Compiling model...\")\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to pull in the raw data here to do the training on\n",
    "\n",
    "hist = model.fit(X, train_target, validation_split=0.2)\n",
    "print(\"\")\n",
    "print(hist.history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
