{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is to develop a prototype classifier based on the MonkeyLearn blog post using GCconnex blog data.  Copy made of LDA-Blogs.\n",
    "\n",
    "https://blog.monkeylearn.com/creating-machine-learning-models-to-analyze-startup-news/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sqlalchemy as sq\n",
    "import pymysql\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "sq.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_unixtime(stamp):\n",
    "    return dt.datetime.fromtimestamp(\n",
    "        int(stamp)\n",
    "    ).strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_snakecase(text_list):\n",
    "    \n",
    "    new_word_list = []\n",
    "    \n",
    "    for words in text_list:\n",
    "        lower_words = words.lower().split()\n",
    "        text = \"_\".join(lower_words)\n",
    "        new_word_list.append(text)\n",
    "        \n",
    "    return new_word_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open(os.path.join(data_path, \"{}.pkl\".format(name)), 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(os.path.join(data_path, \"{}.pkl\".format(name)), 'rb') as f:\n",
    "        return(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '/home/chris/data/'\n",
    "output_path = '/home/chris/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to DB and pull info from GCconnex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "password = getpass.getpass('Enter Password: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MariahDB = 165\n",
    "# MYSQL = 117\n",
    "\n",
    "db_connection = \"mysql+pymysql://gctoolsdata:{}@192.168.1.99:3306/elgg\".format(\n",
    "    password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "engine = sq.create_engine(db_connection,encoding='latin1', echo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conn = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sqlalchemy.orm import sessionmaker, relationship\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sqlalchemy import and_, or_\n",
    "Session = sessionmaker(bind=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Session.configure(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Base = automap_base()\n",
    "\n",
    "Base.prepare(engine, reflect=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up mappings\n",
    "\n",
    "Users = Base.classes.elggusers_entity\n",
    "Groups = Base.classes.elgggroups_entity\n",
    "Relationships = Base.classes.elggentity_relationships\n",
    "Entities = Base.classes.elggentities\n",
    "Objects = Base.classes.elggobjects_entity\n",
    "MetaData = Base.classes.elggmetadata\n",
    "MetaStrings = Base.classes.elggmetastrings\n",
    "Annotations = Base.classes.elggannotations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guide to Elgg Entities\n",
    "\n",
    "Blogs = Entities(subtype=5)\n",
    "Group_Members = Users(relationship=member)\n",
    "Discussions = Entities(subtype=7)\n",
    "Pages = Entities(subtype=10)\n",
    "Wire = Entities(subtype=17)\n",
    "\n",
    "Content = Entities(subtype) -> entity_guid\n",
    "    Elggmetadata(entity_guid) -> name_id, value_id\n",
    "    Elggmetastrings(name_id OR value_id)\n",
    "    \n",
    "#Comments\n",
    "Blog is container entity - GUID = blog guid\n",
    "\n",
    "Blog guid = 10\n",
    "search container for blog guid, return container guid\n",
    "elggmetadata(container_guid)\n",
    "Elggmetastrings(name_id OR value_id)\n",
    "\n",
    "#Skills\n",
    "user_GUID -> elggmetadata(container_guid) - name_id = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up subtype objects of interest\n",
    "\n",
    "subtypes = {'blogs': 5,\n",
    "            'discussions': 7,\n",
    "            'pages': 10,\n",
    "            'wires': 17,\n",
    "            'files': 1,\n",
    "            'images': 19,\n",
    "            'bookmarks': 8,\n",
    "            'ideas': 42\n",
    "           }\n",
    "\n",
    "subtype_list = \"5 7 10 17 1 19 8 42\".split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Pull Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UsersObject(object):  # Pulls in the entire users database\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_all():  # Grabs entire table\n",
    "\n",
    "        user_query = session.query(Users).statement\n",
    "\n",
    "        users = pd.read_sql(user_query, conn)\n",
    "\n",
    "        users['last_action'] = users['last_action'].apply(convert_unixtime)\n",
    "        users['prev_last_action'] = users['prev_last_action'].apply(convert_unixtime)\n",
    "        users['last_login'] = users['last_login'].apply(convert_unixtime)\n",
    "        users['prev_last_login'] = users['prev_last_login'].apply(convert_unixtime)\n",
    "        return users\n",
    "\n",
    "    def filter_department(filter_condition):\n",
    "        users_session = session.query(Users)\n",
    "        users = pd.read_sql(\n",
    "            users_session.filter(\n",
    "                text(\"{}\".format(filter_condition))\n",
    "            ).statement, conn\n",
    "        )\n",
    "\n",
    "        users['last_action'] = users['last_action'].apply(convert_unixtime)\n",
    "        users['prev_last_action'] = users['prev_last_action'].apply(convert_unixtime)\n",
    "        users['last_login'] = users['last_login'].apply(convert_unixtime)\n",
    "        users['prev_last_login'] = users['prev_last_login'].apply(convert_unixtime)\n",
    "\n",
    "        return users\n",
    "    \n",
    "    def department():  # Issue : doesn't pull all members. That's bad.\n",
    "\n",
    "        statement = session.query(\n",
    "            Users.guid,\n",
    "            Users.name,\n",
    "            Users.email,\n",
    "            Users.last_action,\n",
    "            Users.prev_last_action,\n",
    "            Users.last_login,\n",
    "            Users.prev_last_login,\n",
    "            Entities.time_created,\n",
    "            MetaStrings.string\n",
    "        )\n",
    "\n",
    "        statement = statement.filter(MetaStrings.id == MetaData.value_id)\n",
    "        statement = statement.filter(MetaData.name_id == 8667)\n",
    "        statement = statement.filter(MetaData.entity_guid == Users.guid)\n",
    "        statement = statement.filter(Entities.guid == Users.guid)\n",
    "        statement = statement.statement\n",
    "\n",
    "        users_department = pd.read_sql(statement, conn)\n",
    "\n",
    "        users_department['last_action'] = users_department['last_action'].apply(convert_unixtime)\n",
    "        users_department['prev_last_action'] = users_department['prev_last_action'].apply(convert_unixtime)\n",
    "        users_department['last_login'] = users_department['last_login'].apply(convert_unixtime)\n",
    "        users_department['prev_last_login'] = users_department['prev_last_login'].apply(convert_unixtime)\n",
    "        users_department['time_created'] = users_department['time_created'].apply(convert_unixtime)\n",
    "        users_department['organization'] = users_department['string']\n",
    "\n",
    "        return users_department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users = UsersObject.department()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Pull Blogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test for pulling out blog info\n",
    "# Cut out: entity.guid, entity.subtype, user.name, objects.title, \n",
    "\n",
    "blogs = []\n",
    "\n",
    "for entity, objects in session.query(\n",
    "    Entities, Objects).filter(\n",
    "        Entities.subtype == 5,\n",
    "        Objects.guid == Entities.guid):\n",
    "    blogs.append((objects.guid, objects.title, objects.description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tags = []\n",
    "\n",
    "for entity, data, strings in session.query(\n",
    "    Entities, MetaData, MetaStrings).filter(\n",
    "        Entities.subtype == 5,\n",
    "        Entities.guid == MetaData.entity_guid).filter(\n",
    "        or_ (MetaStrings.id == MetaData.value_id,\n",
    "        MetaStrings.id == MetaData.name_id)):\n",
    "    tags.append((entity.guid, data.name_id, data.value_id, strings.id,\n",
    "                 strings.string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tags[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link tags to blogs and conduct analysis of metadata tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scripts for sorting tags and linking them to guids\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "tag_dict = defaultdict(list)\n",
    "\n",
    "for data in tags:\n",
    "    guid, name_id, value_id, string_id, string = data\n",
    "    if name_id == string_id and string == 'tags':\n",
    "        tag_dict.setdefault(guid, []).append(value_id)\n",
    "        \n",
    "strings = {}\n",
    "        \n",
    "for data in tags:\n",
    "    guid, name_id, value_id, string_id, string = data\n",
    "    strings[string_id] =  string\n",
    "    \n",
    "\n",
    "def replace_string_id(tag_list):\n",
    "    return [strings.get(t).lower() for t in tag_list]\n",
    "\n",
    "final_tags = defaultdict(list)\n",
    "\n",
    "for k, v in tag_dict.items():\n",
    "    final_tags[k] = convert_snakecase(replace_string_id(v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processed_tags = defaultdict(list)\n",
    "\n",
    "for k,v in final_tags.items():\n",
    "    if v == ['']:\n",
    "        pass\n",
    "    else:\n",
    "        processed_tags[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_obj(communities, \"communities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count the frequency of each tag from our text\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "tag_frequency = defaultdict(int)\n",
    "\n",
    "for item in final_tags:\n",
    "    for tag in final_tags[item]:\n",
    "        tag_frequency[tag] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_freq = pd.DataFrame.from_dict(tag_frequency, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_freq.columns = ['frequency']\n",
    "tag_freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_freq.sort_values(by='frequency', inplace=True, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "tag_freq.head(50).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_freq.to_csv(os.path.join('~/data/', 'blog_tags_2017_04_25.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a sorted dictionary based on the frequency\n",
    "\n",
    "sorted_tag_freq = OrderedDict(sorted(tag_frequency.items(),\n",
    "                                key=lambda kv: kv[1],\n",
    "                                reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_tag_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Set communities using tags from Information Architecture Review\n",
    "\n",
    "Martin to enter info here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Categories of tags for community identification\n",
    "\n",
    "ATIP = ['access to information', 'atip', 'privacy', 'censorship',\n",
    "       'confidentiality', 'freedom of information', 'information requests',\n",
    "       'open government', 'data breach', 'right to privacy', 'right to privacy',\n",
    "       'security breach', 'personal information']\n",
    "\n",
    "material_management = ['material management', 'material', 'supply chain management', 'supply chain',\n",
    "                      'inventories', 'logistics', 'supplies']\n",
    "\n",
    "procurement_specialists = ['procurement', 'purchasing', 'acquisition', 'government purchasing',\n",
    "                          'ordering', 'public purchasing', 'buyers', 'consumerism', 'costs',\n",
    "                          'standing offers', 'supplies']\n",
    "\n",
    "real_property = ['property', 'real_property', 'real estate', 'realty', 'expropriation', 'property tax',\n",
    "                'real estate industry', 'real property services']\n",
    "\n",
    "evaluators = ['evaluators', 'assessment', 'appraisal', 'evaluations' ,'grading',\n",
    "             'environmental impact assessment', 'eia', 'performance assessment',\n",
    "             'benchmarks', 'comparison', 'control', 'measurement', 'merit', 'revision', 'testing',\n",
    "             'pr processess', 'project management', 'program review', 'programs']\n",
    "\n",
    "communication = ['communication', 'comms', 'communications', 'government communications', 'military communications',\n",
    "                'telecommunications', 'communications equipment', 'media', 'information and communications',\n",
    "                'information bulletin', 'press releases', 'outreach', 'engagement']\n",
    "\n",
    "regulators = ['regulator', 'regulators', 'regulate', 'legislation', 'licensing', 'regulations', 'economic regulations',\n",
    "             'safety regulations', 'by-laws', 'taxation regulation', 'legislation', 'legislative writing', 'legislative',\n",
    "             'regulation', 'regulatory agencies', 'regulatory agency', 'price regulation']\n",
    "\n",
    "financial_officers = ['financial', 'finance', 'finances', 'financial officers', 'financial management', 'international finance',\n",
    "                     'public finance', 'quarterly financial reports', 'qfa', 'accounting standards', 'budget', 'budgets',\n",
    "                     'chief financial officer', 'cfo', 'cfos', 'financial analysis', 'financial crisis',\n",
    "                     'financial management', 'financial services', 'financial statements', 'portfolio',\n",
    "                     'supplementary estimates', 'financial administration', 'financial planning',\n",
    "                     'fiscal planning', 'money management', 'budget planning', 'expenditure management',\n",
    "                     'financial analysis']\n",
    "\n",
    "information_management = ['information management', 'cataloguing', 'data processing', 'information',\n",
    "                         'information dissemination', 'information policy', 'information systems', 'knowledge management',\n",
    "                         'metadata', ' records management']\n",
    "\n",
    "information_technology = ['information technology', 'technology', 'artificial intelligence', 'computer networks',\n",
    "                         'electronic data interchange', 'intelligent systems', 'multimedia', 'telecommunications',\n",
    "                         ]\n",
    "\n",
    "internal_auditors = ['audit', 'internal audit', 'auditors', 'internal auditors', 'governance', 'review', 'risk management']\n",
    "\n",
    "security_specialist = ['security', 'security specialists', 'secure', 'computer security',\n",
    "                      'human security', 'international security', 'national security', 'safety', 'investigations',\n",
    "                      'safety investigations']\n",
    "\n",
    "human_resources = ['hr', 'human resources', 'human resource', 'ressources humaines', 'rh', 'personnel',\n",
    "                  'staff' ,'chief human resources officer', 'labour force', 'personnel management', 'staffing',\n",
    "                  'workers']\n",
    "\n",
    "policy = ['policy', 'policy specialists', 'agricultural policy', 'cultural policy', 'defence policy', 'economic policy',\n",
    "         'education policy', 'energy policy', 'fiscal policy', 'foreign policy', 'environmental policy',\n",
    "         'fiscal policy', 'fisheries policy', 'food policy', 'foreign policy', 'forestry policy', 'government policy',\n",
    "         'health policy', 'immigration policy', 'industrial policy', 'investment policy', 'language policy',\n",
    "         'monetary policy', 'science policy', 'social policy', 'technology policy', 'policy development',\n",
    "         'policy instruments', 'policy review']\n",
    "\n",
    "fed_science_tech = ['science', 'technology', 'life science', 'life sciences', 'informatics', 'computer science',\n",
    "                   'analytics', 'natural sciences', 'geosciences', 'chemistry', 'geography', 'geology', 'hydrology',\n",
    "                   'hard rocks', 'meteorology', 'scientists' ,'scientist', 'oceanography', 'ecology',\n",
    "                   'earth sciences', 'medicine', 'pharmacology', 'toxicology', 'biology', 'mathematics', 'math',\n",
    "                   'statistics', 'physics', 'social science', 'social sciences', 'astronomy']\n",
    "\n",
    "services = ['secretariat', 'technical services', 'services', 'client', 'service provider', 'client satisfaction',\n",
    "           'user experience', 'design thinking', 'graphical user interface', 'service levels', 'conversions']\n",
    "\n",
    "\n",
    "# Convert all community tags to snakecase\n",
    "\n",
    "'''ATIP = convert_snakecase(ATIP)\n",
    "material_management = convert_snakecase(material_management) \n",
    "procurement_specialists = convert_snakecase(procurement_specialists) \n",
    "real_property = convert_snakecase(real_property) \n",
    "evaluators = convert_snakecase(evaluators) \n",
    "communication = convert_snakecase(communication) \n",
    "regulators = convert_snakecase(regulators) \n",
    "financial_officers = convert_snakecase(financial_officers) \n",
    "information_management = convert_snakecase(information_management) \n",
    "information_technology = convert_snakecase(information_technology) \n",
    "internal_auditors = convert_snakecase(internal_auditors) \n",
    "security_specialist = convert_snakecase(security_specialist) \n",
    "human_resources = convert_snakecase(human_resources) \n",
    "policy = convert_snakecase(policy) \n",
    "fed_science_tech = convert_snakecase(fed_science_tech)'''\n",
    "\n",
    "\n",
    "# Create list of communities to iterate through\n",
    "\n",
    "communities = {'ATIP' : {'name': 'ATIP', 'tags': ATIP},\n",
    "               'material_management' : {'name': 'material_management', 'tags': material_management},\n",
    "               'procurement_specialists' : {'name': 'procurement_specialists', 'tags': procurement_specialists},\n",
    "               'real_property' : {'name': 'real_property', 'tags': real_property},\n",
    "               'evaluators' : {'name': 'evaluators', 'tags': evaluators},\n",
    "               'communication' : {'name': 'communication', 'tags': communication},\n",
    "               'regulators' : {'name': 'regulators', 'tags': regulators},\n",
    "               'financial_officers' : {'name': 'financial_officers', 'tags': financial_officers},\n",
    "               'information_management' : {'name': 'information_management', 'tags': information_management},\n",
    "               'information_technology' : {'name': 'information_technology', 'tags': information_technology},\n",
    "               'internal_auditors' : {'name': 'internal_auditors', 'tags': internal_auditors},\n",
    "               'security_specialist' : {'name': 'security_specialist', 'tags': security_specialist},\n",
    "               'human_resources' : {'name': 'human_resources', 'tags': human_resources},\n",
    "               'policy' : {'name': 'policy', 'tags': policy},\n",
    "               'fed_science_tech' : {'name': 'fed_science_tech', 'tags': fed_science_tech}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data from blogs and prepare for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Beautifulsoup to remove HTML tags\n",
    "# Langdetect to ... detect languages\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect, detect_langs\n",
    "blog_info = []\n",
    "\n",
    "for blog in blogs:\n",
    "    guid, name, description = blog\n",
    "    name = BeautifulSoup(name, \"lxml\")\n",
    "    description = BeautifulSoup(description, \"lxml\")\n",
    "    language = detect(description)\n",
    "    tags = processed_tags.get(guid, \"None\")\n",
    "    blog_info.append([guid, name.text, description.text, tags, language])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blog_info[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blog_df = pd.DataFrame(blog_info, columns=['guid', 'title', 'content', 'tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blog_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blog_df.to_csv(\"gcconnex_blogs_info.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Break.  Still need to fix and clean tags to communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reload dataframe\n",
    "\n",
    "blogs = pd.DataFrame.from_csv(os.path.join(data_path, \"gcconnex_blogs_info.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blogs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reload community dict\n",
    "\n",
    "#community_dict = load_obj(\"gcconnex_blogs_communities_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#community_dict[11303]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Pre-Processing\n",
    "\n",
    "Using Gensim and NLTK to tokenzied, lemmatize and clean text for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import gensim\n",
    "import bz2\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess, lemmatize\n",
    "from gensim.parsing.preprocessing import STOPWORDS as STOPWORDS\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# create French stop word list\n",
    "fr_stops = set(stopwords.words('french'))\n",
    "\n",
    "# Add public service specific stopwords - we could expand this, but the algorithms will do\n",
    "# a lot of that for us\n",
    "\n",
    "public_service_stops = '''public service canada work http \n",
    "https travail gcconnex url'''.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Stemming example - need to set up Stemmer for French as well\n",
    "'''\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', \n",
    "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n",
    "           'traditional', 'reference', 'colonizer','plotted']\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data Pre-processing\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    # our lemmatizer - is called in tokenize\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def tokenize(text):\n",
    "    # our tokenizer\n",
    "    return [lemmatize_stemming(token) for token in tokenizer.tokenize(str(text))\n",
    "            if token not in STOPWORDS if token not in fr_stops\n",
    "           if token not in public_service_stops if len(token) > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pre-process blog content (takes a while)\n",
    "\n",
    "blogs['content_tokens'] = blogs.content.apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-process blog tags\n",
    "\n",
    "blogs['tag_tokens'] = blogs['tags'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blogs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tag_dictionary = corpora.Dictionary(blogs.tag_tokens) # could include prune_at=2000\n",
    "content_dictionary = corpora.Dictionary(blogs.content_tokens) # could include prune_at=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "content_dictionary.save(os.path.join(\n",
    "    '/home/chris/data/', 'gcconnex_blogs_content_dictionary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_dictionary.save(os.path.join(\n",
    "    '/home/chris/data/', 'gcconnex_blogs_tags_dictionary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Have a look at one of our dictionaries (key=#, value=word)\n",
    "\n",
    "count = 0\n",
    "for k,v in content_dictionary.items():\n",
    "    print(k,v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove content that either appears too frequently to matter or too rarely.\n",
    "\n",
    "content_dictionary.filter_extremes(no_below=15, no_above=0.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform content into bag-of-words\n",
    "\n",
    "content_bow_corpus = [content_dictionary.doc2bow(blog) for blog in blogs.content_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "content_bow_corpus[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Preview BOW for our sample preprocessed document\n",
    "'''\n",
    "\n",
    "def preview_bow(doc, dictionary):\n",
    "    # given a document and dictionary, creates a preview of the bow model\n",
    "    for i in range(len(doc)):\n",
    "        print(\"Word {} (\\\"{}\\\") appears {} time.\".format(doc[i][0], \n",
    "                                                         dictionary[doc[i][0]], \n",
    "                                                         doc[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Checking out our bag-of-words\n",
    "\n",
    "bow_doc_17 = content_bow_corpus[17]\n",
    "bow_doc_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using preview-BOW to get more human readable output\n",
    "\n",
    "preview_bow(bow_doc_17, content_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculating the overall term frequency and importance using TF-idf\n",
    "\n",
    "# First we create the model based on the overall corpus\n",
    "\n",
    "content_tfidf = models.TfidfModel(content_bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Then we apply the model to each document in the corpus\n",
    "\n",
    "content_corpus_tfidf = content_tfidf[content_bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Preview TF-IDF scores for our first document --> --> (token_id, tfidf score)\n",
    "'''\n",
    "\n",
    "from pprint import pprint\n",
    "for doc in content_corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Latent variables - LDA\n",
    "\n",
    "Looking for communities in the overall text based on Latent Dirichlet Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LDA multicore \n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(content_bow_corpus, \n",
    "                                       num_topics=20, \n",
    "                                       id2word = content_dictionary,\n",
    "                                       workers=7,\n",
    "                                       passes = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "'''\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} Word: {}\".format(idx, topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Define lda model using tfidf corpus\n",
    "'''\n",
    "content_lda_model_tfidf = gensim.models.LdaMulticore(content_corpus_tfidf, \n",
    "                                             num_topics=20, \n",
    "                                             id2word = content_dictionary, \n",
    "                                             passes = 50, \n",
    "                                             workers=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "'''\n",
    "for idx, topic in content_lda_model_tfidf.print_topics(-1):\n",
    "    print(\"Topic: {} Word: {}\".format(idx, topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_obj(lda_model, \"gcconnex_blogs_lda_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_obj(content_lda_model_tfidf, \"gcconnex_blogs_tfidf_lda_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like the regular LDA is giving better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Start bow for communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transform communities into list (easier parsing) and create a list of labels\n",
    "\n",
    "community_list = []\n",
    "community_labels = []\n",
    "\n",
    "for community in communities.items():\n",
    "    name, data = community\n",
    "    title, tags = data.values()\n",
    "    community_list.append(\" \".join(tags))\n",
    "    community_labels.append(title)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(f\"{community_labels[2]}: {community_list[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenize and lemmatize community tags\n",
    "\n",
    "community_tokens = []\n",
    "\n",
    "for tags in community_list:\n",
    "    community_tokens.append([lemmatize_stemming(token) for token in tokenizer.tokenize(tags)])\n",
    "    \n",
    "community_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save our stuff\n",
    "\n",
    "community_tags_dictionary = corpora.Dictionary(community_tokens) # could include prune_at=2000\n",
    "\n",
    "community_tags_bow_corpus = [community_tags_dictionary.doc2bow(tokens) for tokens in community_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "community_tags_dictionary.save(os.path.join(data_path, \"community_tags_dictionary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "community_tags_bow_corpus[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "document = community_tags_bow_corpus[4]\n",
    "\n",
    "preview_bow(document, community_tags_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to figure a mapping from the content_tags to the community_tags.\n",
    "\n",
    "For each set of content tags:\n",
    "* see if the tag is in the community tags\n",
    "* if yes, add community label & counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blog_tokens = blogs.content_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blog_tokens[1][:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Identify communities via matching user generated tags against pre-defined community tags\n",
    "\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def identify_community(tag_list):\n",
    "    \n",
    "    community_list = defaultdict(int)\n",
    "    \n",
    "    # pre-populate community list with communities\n",
    "    \n",
    "    for community in communities:\n",
    "                        \n",
    "        c = communities.get(community)\n",
    "        community_list[c['name']] = 0\n",
    "    \n",
    "    if isinstance(tag_list, list):\n",
    "    \n",
    "        for tag in tag_list:\n",
    "\n",
    "                for community in communities:\n",
    "\n",
    "                    c = communities.get(community)\n",
    "\n",
    "                    for t in c['tags']:\n",
    "\n",
    "                        if re.search(tag, t):\n",
    "                            community_list[c['name']] += 1\n",
    "\n",
    "    return community_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_doc = blog_tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(test_doc)\n",
    "identify_community(test_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blog_communities = []\n",
    "\n",
    "for blog in blog_tokens:\n",
    "    blog_communities.append(identify_community(blog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blog_communities[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_obj(blog_communities, \"blog_communities\") # List\n",
    "save_obj(blog_tokens, \"blog_tokens\") # List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform blog_communities into labels we can predict against\n",
    "\n",
    "community_labels = []\n",
    "\n",
    "for blog in blog_communities:\n",
    "    new_list = []\n",
    "    for k,v in blog.items():\n",
    "        new_list.append(v)\n",
    "        \n",
    "    community_labels.append(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "community_labels[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalize the values so that we can use them as predictors\n",
    "# Arbitrary number of 4 chosen here, but let's see what happens\n",
    "\n",
    "def binarize(lst):\n",
    "    new_list = []\n",
    "    for item in lst:\n",
    "        if item >= 4:\n",
    "            new_list.append(1)\n",
    "        else:\n",
    "            new_list.append(0)\n",
    "            \n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create array of binarized labels\n",
    "\n",
    "binarized_labels = np.array([binarize(labels) for labels in community_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "binarized_labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_obj(binarized_labels, \"binarized_labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''# Setting up for labelling blogs\n",
    "\n",
    "for blog, community_info in zip(new_blogs, communities_list):\n",
    "    \n",
    "    if community_info:\n",
    "        for community in communities:\n",
    "            c = community_info.get(community, 0)\n",
    "            blog[community] = c\n",
    "    else:\n",
    "        for community in communities:\n",
    "            blog[community] = 0\n",
    "\n",
    "\n",
    "# Add rows for each community to the DF\n",
    "\n",
    "for community in communities:\n",
    "    c = communities.get(community)\n",
    "    blogs[c['name']] = 0'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "community_names = []\n",
    "\n",
    "for community in communities:\n",
    "                        \n",
    "        c = communities.get(community)\n",
    "        community_names.append(c['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "community_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay. We now have blogs and their associated communities with absolute strengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to put blogs back together for vectorizer to work\n",
    "# FIX THIS\n",
    "\n",
    "blogs_joined = [\" \".join(tokens) for tokens in blog_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blogs_joined[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_counts = count_vec.fit_transform(blogs_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_vec.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_obj(count_vec, \"count_vectorizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier - trying out one-vs-the-rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train_counts, binarized_labels, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up our multi-label classifier\n",
    "\n",
    "multilabel_clf = OneVsRestClassifier(SVC(probability=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit our classifier to the data\n",
    "\n",
    "multilabel_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Quick test of our accuracy score against our test data\n",
    "\n",
    "multilabel_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cross Val score runs multiple tests against segments of our data\n",
    "# It also takes a long time\n",
    "\n",
    "scores = cross_val_score(\n",
    "    multilabel_clf, X_train_counts, binarized_labels, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy: {} (+/- {})\".format(scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted = multilabel_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.classification_report(y_test, predicted,\n",
    "                                    target_names=community_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Save classifier \n",
    "\n",
    "save_obj(multilabel_clf, \"multilabel_clf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "multilabel_clf = load_obj(\"multilabel_clf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "multilabel_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test new blog\n",
    "\n",
    "test_blog = '''I think innovation in the Public Service these days is fairly immature. \n",
    "So I think about a fairly disjointed community (if you can call it that) still trying to come \n",
    "to terms with what innovative public sector organizations even look like. I don't actually think \n",
    "we're alone though, as work the OECD is doing on innovation skills suggests many other jurisdictions \n",
    "are where we are. But I think our understanding of the value is fairly immature. \n",
    "I also think it'd be good to separate out innovation as a discipline (\"we're doing that \n",
    "thing using innovation practices\") and innovation as a product (\"that thing we \n",
    "designed/built/implemented is innovative\"). In the public service, I find we are generally \n",
    "quick to label our products as innovative (\"that dragon's den we held was innovative\"), but \n",
    "we haven't spent enough time developing innovation as a discipline (\"we solved that problem \n",
    "by applying X innovative practice/method\"). There's where I see the hubs/labs potentially adding \n",
    "value. But even there, I think we have a long way to go. Glad to see some work is underway to try \n",
    "to make sense of it all though. :)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_tokens = tokenize(test_blog)\n",
    "test_tokens = \" \".join(test_tokens)\n",
    "test_tokens_count = count_vec.transform([test_tokens])\n",
    "test_tokens_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk_rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rake = nltk_rake.RakeKeywordExtractor()\n",
    "keywords = rake.extract(test_blog, incl_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isNumeric(word):\n",
    "  try:\n",
    "    float(word) if '.' in word else int(word)\n",
    "    return True\n",
    "  except ValueError:\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = \"1 2 3 yes no maybe\".split()\n",
    "\n",
    "len([x for x in y if not isNumeric(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blueprint = '''\n",
    "Blueprint 2020 is a vision for a world-class Public Service equipped to serve Canada and Canadians now and into the future.\n",
    "\n",
    "With around a quarter-million employees, the Public Service of Canada is the largest employer in the country. We work across more than a hundred departments and agencies, delivering important services to Canadians  from providing old age security and employment insurance benefits to protecting Canadian sovereignty to helping families save for higher education. We are responsible for regulating the safety of food and drugs, undertaking research and development to protect our shared environment, promoting Canadas national interests around the world, and developing economic, trade and energy policies, among many other duties. We make a difference in the lives of Canadians every day. To ensure continued excellence in public service requires us to always ask how we can improve both our performance and our value to Canadians. The Blueprint 2020 initiative was devised to help us ask these questions, and to allow us to build tomorrows Public Service together.\n",
    "\n",
    "Since June 2013, tens of thousands of public servants have shared their views on what it takes to ensure public service excellence. This input is redefining how we work and is making engagement part of our shared culture.\n",
    "\n",
    "A team of employees working at the Canada Border Service Agency\n",
    "By making a space to discuss our passion for public service and to act on our ideas for improvement, we are now well on our way to turning vision into reality. Building the Public Service of the future is a process that requires us all to commit to action, and to dedicate the time and effort necessary to see it through. We are all personally accountable for bringing about real change and realizing our ambitious goals.\n",
    "\n",
    "For some examples of innovations that have arisen as a result of engaged employees working together to deliver results for Canadians, please check out the Clerk of the Privy Councils latest Annual Report to the Prime Minister on the Public Service of Canada.\n",
    "\n",
    "Blueprint 2020 Principles:\n",
    "The Blueprint 2020 vision is guided by four principles, as outlined in the document Blueprint 2020: Getting Started  Getting Your Views, that help examine how work is done in the federal Public Service:\n",
    "\n",
    "An open and networked environment that engages citizens and partners for the public good.\n",
    "A whole-of-government approach that enhances service delivery and value for money.\n",
    "A modern workplace that makes smart use of new technologies to improve networking, access to data and customer service.\n",
    "A capable, confident and high-performing workforce that embraces new ways of working and mobilizes the diversity of talent to serve the countrys evolving needs.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bp_tokens = tokenize(blueprint)\n",
    "bp_tokens = \" \".join(bp_tokens)\n",
    "bp_tokens_count = count_vec.transform([bp_tokens])\n",
    "bp_tokens_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_obj(community_names, \"community_names\") # list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted = multilabel_clf.predict(test_tokens_count)\n",
    "predicted_prob = multilabel_clf.predict_proba(test_tokens_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(predicted)\n",
    "print(predicted_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_communities(predict_array):\n",
    "    for i, element in enumerate(np.nditer(predict_array)):\n",
    "        print(\"{}: {}\".format(community_names[i], element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_communities(predicted_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# Set up pipeline for new models\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', multilabel_clf)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frequency = defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove words that occur less than 5 times and than have less than 3 letters\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 5 if len(token) > 3]\n",
    "         for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count the frequency of each token from our text\n",
    "\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a sorted dictionary based on the frequency\n",
    "\n",
    "sorted_freq = OrderedDict(sorted(frequency.items(),\n",
    "                                key=lambda kv: kv[1],\n",
    "                                reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_model = models.tfidfmodel.TfidfModel(\n",
    "    corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('blogs.csv', 'w+', encoding='latin-1') as f:\n",
    "    for blog in blogs:\n",
    "        f.write(str(blog))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prep for NLTK analysis\n",
    "\n",
    "full_text = \"\\n\".join(str(blogs))\n",
    "\n",
    "\n",
    "tokens = word_tokenize(full_text)\n",
    "text = nltk.Text(tokens)\n",
    "sens = nltk.sent_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_long(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_trigrams(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
